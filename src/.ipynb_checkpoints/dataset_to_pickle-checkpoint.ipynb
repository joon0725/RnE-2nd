{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "043e376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "import os.path\n",
    "\n",
    "from dev.preprocessing import faceMesh_video\n",
    "from dev.preprocessing import handPose_video\n",
    "from dev.preprocessing import points_to_displacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf117390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_num = [0, 7, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58, 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105, 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 172, 173, 176, 178, 181, 185, 191, 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291, 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324, 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409, 415, 454, 466]\n",
    "len(points_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6304462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f79caf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19번째 단어의 0번째 영상, 00_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 1번째 영상, 01_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 2번째 영상, 02_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 3번째 영상, 03_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 4번째 영상, 04_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 5번째 영상, 05_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 6번째 영상, 06_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 7번째 영상, 07_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 8번째 영상, 08_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 9번째 영상, 09_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 10번째 영상, 10_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 11번째 영상, 11_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 12번째 영상, 12_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 13번째 영상, 13_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 14번째 영상, 14_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 15번째 영상, 15_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 16번째 영상, 16_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 17번째 영상, 17_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 18번째 영상, 18_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "19번째 단어의 19번째 영상, 19_19.MP4 는 존재하지 않습니다. Skipping...\n",
      "20번째 단어의 0번째 영상, 00_20.MP4 처리 시작.\n",
      "20번째 단어의 0번째 영상, 00_20.MP4 처리 완료.\n",
      "\n",
      "20번째 단어의 1번째 영상, 01_20.MP4 처리 시작.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m번째 단어의 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m번째 영상, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(j)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.MP4 처리 시작.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m face_points \u001b[38;5;241m=\u001b[39m faceMesh_video(i, j)\n\u001b[0;32m---> 14\u001b[0m hand_points \u001b[38;5;241m=\u001b[39m \u001b[43mhandPose_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m point_displacements \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# 모든 점에 대한 변위 데이터 저장\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 한 영상에서의 변위데이터 딕셔너리 배열 가져옴\u001b[39;00m\n",
      "File \u001b[0;32m~/RnE-2nd/src/dev/preprocessing.py:181\u001b[0m, in \u001b[0;36mhandPose_video\u001b[0;34m(wnum, seq)\u001b[0m\n\u001b[1;32m    178\u001b[0m     handPose_payload\u001b[38;5;241m.\u001b[39mappend(handPose_payload[idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:  \u001b[38;5;66;03m# case 2: 프레임은 있는데 손 인식이 아예 안됨\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mediapipe/python/solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    362\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    363\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('../dataset/processed/KSL_handpose_and_facemesh.p', 'wb') as file:\n",
    "    dataset_pickle = []\n",
    "\n",
    "    face_count = 128\n",
    "    hand_count = 21\n",
    "    for i in range(1, 78):\n",
    "        for j in range(20):\n",
    "            if (not os.path.isfile(f\"../dataset/{str(i).zfill(2)}/{str(j).zfill(2)}_{str(i).zfill(2)}.MP4\")):\n",
    "                print(f'{i}번째 단어의 {j}번째 영상, {str(j).zfill(2)}_{str(i).zfill(2)}.MP4 는 존재하지 않습니다. Skipping...')\n",
    "                continue\n",
    "            print(f'{i}번째 단어의 {j}번째 영상, {str(j).zfill(2)}_{str(i).zfill(2)}.MP4 처리 시작.')\n",
    "\n",
    "            face_points = faceMesh_video(i, j)\n",
    "            hand_points = handPose_video(i, j)\n",
    "            \n",
    "            point_displacements = [] # 모든 점에 대한 변위 데이터 저장\n",
    "            \n",
    "            # 한 영상에서의 변위데이터 딕셔너리 배열 가져옴\n",
    "            displacement = points_to_displacement(face_points, face_count, hand_points, hand_count)\n",
    "\n",
    "            for per_frame_data in displacement:\n",
    "                temp = np.concatenate([per_frame_data['face'], per_frame_data['hands']['left'], per_frame_data['hands']['right']], axis=0)\n",
    "                point_displacements.append(temp)\n",
    "            \n",
    "            point_displacements = np.array(point_displacements)\n",
    "\n",
    "            video_data = [point_displacements, i]\n",
    "\n",
    "            dataset_pickle.append(video_data)\n",
    "            print(f'{i}번째 단어의 {j}번째 영상, {str(j).zfill(2)}_{str(i).zfill(2)}.MP4 처리 완료.')\n",
    "            print()\n",
    "\n",
    "    pickle.dump(dataset_pickle, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22039296",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16262e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
